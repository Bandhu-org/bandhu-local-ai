import streamlit as st
from rag_engine import get_context
from ollama_client import query_ollama

st.set_page_config(page_title="Bandhu Local AI", page_icon="ðŸªµ")

st.title("Bandhu â€” Copilote Local")
question = st.text_input("Pose une question Ã  ton IA mÃ©tier :")

if question:
    with st.spinner("Recherche en cours..."):
        context, sources = get_context(question)
        prompt = f"""Tu es un assistant mÃ©tier local. 
Voici le contexte extrait des documents internes :\n\n{context}\n\n
RÃ©ponds Ã  la question : {question}
        """
        response = query_ollama(prompt)

        st.markdown("### RÃ©ponse de l'IA :")
        st.write(response)

        st.markdown("#### Source(s) :")
        for src in sources:
            st.code(src.get("source", "inconnu"))
